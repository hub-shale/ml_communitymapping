---
title: 'Community: Cluster'
author: "Shale"
date: "1/31/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# load R packages
librarian::shelf(
  dplyr, DT, ggplot2, palmerpenguins, skimr, tibble, ggvoronoi, scales, cluster, vegan, vegan3d, factoextra, h2o)

# set seed for reproducible results
set.seed(60)

# load the dataset
data("penguins")

# remove the rows with NAs
penguins <- na.omit(penguins)
```

# 2a: Clusters

## `kmeans()`

```{r}
# plot petal length vs width, species naive
ggplot(
  penguins, aes(bill_length_mm, bill_depth_mm)) +
  geom_point()
```

```{r}
# plot petal length vs width, color by species
legend_pos <- theme(
    legend.position = c(0.95, 0.05),
    legend.justification = c("right", "bottom"),
    legend.box.just = "right")
ggplot(
  penguins, aes(bill_length_mm, bill_depth_mm, color = species)) +
  geom_point() +
  legend_pos
```

```{r}
# cluster using kmeans
k <- 3  # number of clusters
penguins_k <- kmeans(
  penguins %>% 
    select(bill_length_mm, bill_depth_mm), 
  centers = k)

# show cluster result
penguins_k
```

#### Question: Comparing the observed species plot with 3 species with the kmeans() cluster plot with 3 clusters, where does this “unsupervised” kmeans() technique (that does not use species to “fit” the model) produce similar versus different results? One or two sentences would suffice. Feel free to mention ranges of values along the axes.

**Looking at the cluster means table, group 1 seems to include mostly Gentoo penguins (though Gentoos with generally shorter and narrower bills), but also includes the lower end of Chinstraps. The rest of the Chinstraps and the Gentoos with the longest and widest beaks are lumped into group 2, and group 3 seems to pretty well capture the Adelies, plus one (as indicated in the table below) of the Chinstraps with the shortest beaks.**

```{r}
# compare clusters with species (which were not used to cluster)
table(penguins_k$cluster, penguins$species)
```
This plot visualizes the groups: it is easy to see that the `kmeans()` technique here favors grouping by `bill_length_mm` rather than `bill_depth_mm`.
```{r}
# extract cluster assignment per observation
Cluster = factor(penguins_k$cluster)

ggplot(penguins, aes(bill_length_mm, bill_depth_mm, color = Cluster)) +
  geom_point() + 
  legend_pos
```

### Voroni Diagram (functionized)

```{r}
# define bounding box for geom_voronoi()
xr <- extendrange(range(penguins$bill_length_mm), f=0.1)
yr <- extendrange(range(penguins$bill_depth_mm), f=0.1)
box <- tribble(
  ~bill_length_mm, ~bill_depth_mm, ~group,
  xr[1], yr[1], 1,
  xr[1], yr[2], 1,
  xr[2], yr[2], 1,
  xr[2], yr[1], 1,
  xr[1], yr[1], 1) %>% 
  data.frame()
```


```{r}
# function to make a voroni diagram for a kmeans() model of the palmer penguins dataset given k clusters
voroni_d_k <- function(k) {
  
penguins_k <- kmeans(
  penguins %>% 
    select(bill_length_mm, bill_depth_mm), 
  centers = k)

# extract cluster assignment per observation
Cluster = factor(penguins_k$cluster)

# extract cluster centers
ctrs <- as.data.frame(penguins_k$centers) %>% 
  mutate(
    Cluster = factor(1:k))

# plot points with voronoi diagram showing nearest centroid
ggplot(penguins, aes(bill_length_mm, bill_depth_mm, color = Cluster)) +
  geom_point() + 
  legend_pos +
  geom_voronoi(
    data = ctrs, aes(fill=Cluster), color = NA, alpha=0.5, 
    outline = box) + 
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  geom_point(data = ctrs, pch=23, cex=2, fill="black")
}
```

```{r}
voroni_d_k(2)
voroni_d_k(3)
voroni_d_k(8)
```

## Hierarcical Clustering

#### 	Question: In your own words, how does Bray Curtis differ from Euclidean distance? See sites_euclidean versus sites_bray from lab code, slides from Lecture 05. Clustering and reading Chapter 8 of Kindt and Coe (2005).

Euclidean distiance is what I would call simple geometric distance (the distance between points A and B, for example by using the Pythagorean Theorem), whereas Bray-Curtis distance is a measure of proportional distance between 0 and 1 (1=different, 0=same) that takes into account the number of observations of the less-common species for each site as a proportion of total observations of all species across all sites.

```{r}
# load dune dataset from package vegan
data("dune")

sites <- tribble(
  ~site, ~sp1, ~sp2, ~sp3,
    "A",    1,    1,    0,
    "B",    5,    5,    0,
    "C",    0,    0,    1) %>% 
  column_to_rownames("site")
sites
```

Distance measures:

```{r}
sites_manhattan <- vegdist(sites, method="manhattan")
sites_manhattan

sites_euclidean <- vegdist(sites, method="euclidean")
sites_euclidean

sites_bray <- vegdist(sites, method="bray")
sites_bray
```


```{r}
# Dissimilarity matrix
d <- vegdist(dune, method="bray")
as.matrix(d)[1:5, 1:5]
```


```{r}
# Hierarchical clustering using Complete Linkage
hc1 <- hclust(d, method = "complete" )

# Dendrogram plot of hc1
plot(hc1, cex = 0.6, hang = -1)

# Compute agglomerative clustering with agnes
hc2 <- agnes(dune, method = "complete")

# Agglomerative coefficient
hc2$ac

# Dendrogram plot of hc2
plot(hc2, which.plot = 2)
```

#### Question: Which function comes first, vegdist() or hclust(), and why? 

**`vegdist()` comes first because `hclust()` needs to know the distance/dissimilarity index between sites as input in order to cluster them, which is calculated in `vegdist()`.**

#### 	Question: In your own words how does hclust() differ from agnes()?

**`hclust()` and `agnes()` are both agglomerative clustering functions, meaning they start with each site as its own cluster and then join clusters together based on similarity. The main difference is that `agnes()` provides the agglomerative coefficient, a measure of the amount of clustering structure.**

```{r}
# methods to assess
m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")

# function to compute coefficient
ac <- function(x) {
  agnes(dune, method = x)$ac
}

# get agglomerative coefficient for each linkage method
purrr::map_dbl(m, ac)
```

#### Question: Of the 4 methods, which is the “best” model in terms of Agglomerative Coefficient?

**The ward method is the best, judging by the agglomerative coefficient.**

```{r}
# compute divisive hierarchical clustering
hc4 <- diana(dune)

# Divise coefficient; amount of clustering structure found
hc4$dc
```

#### Question: In your own words how does agnes() differ from diana()? 

**`diana()` is a divisive clustering method, which starts with all sites in one cluster then splits them up by dissimilarity (as opposed to `agnes()`, which is agglomerative). `diana()` provides the divisive coefficient, which can be used similarly to the agglomerative coefficient.**

```{r}
# Plot cluster results
p1 <- fviz_nbclust(dune, FUN = hcut, method = "wss",  k.max = 10) +
  ggtitle("(A) Elbow method")

p2 <- fviz_nbclust(dune, FUN = hcut, method = "silhouette", k.max = 10) +
  ggtitle("(B) Silhouette method")

p3 <- fviz_nbclust(dune, FUN = hcut, method = "gap_stat", k.max = 10) +
  ggtitle("(C) Gap statistic")

# Display plots side by side
gridExtra::grid.arrange(p1, p2, p3, nrow = 1)
```

#### Question: How do the optimal number of clusters compare between methods for those with a dashed line?

**The optimal number of clusters for the silhouette method is 4, while the optimal number of clusters for gap statistic is 3.**

```{r}
# Construct dendorgram for the Ames housing example
hc5 <- hclust(d, method = "ward.D2" )
dend_plot <- fviz_dend(hc5)
dend_data <- attr(dend_plot, "dendrogram")
dend_cuts <- cut(dend_data, h = 8)
fviz_dend(dend_cuts$lower[[2]])
```

```{r}
# Ward's method
hc5 <- hclust(d, method = "ward.D2" )

# Cut tree into 4 groups
k = 4
sub_grp <- cutree(hc5, k = k)

# Number of members in each cluster
table(sub_grp)

# Plot full dendogram
fviz_dend(
  hc5,
  k = k,
  horiz = TRUE,
  rect = TRUE,
  rect_fill = TRUE,
  rect_border = "jco",
  k_colors = "jco")
```


#### Question: In dendrogram plots, which is the biggest determinant of relatedness between observations: the distance between observations along the labeled axes or the height of their shared connection?

**The height of the shared connection is the metric of their relatedness.**

# 2b: Ordination

### Principal Components Analysis

```{r}
# get data
url <- "https://koalaverse.github.io/homlr/data/my_basket.csv"
my_basket <- readr::read_csv(url)
dim(my_basket)

h2o.no_progress()  # turn off progress bars for brevity
h2o.init(max_mem_size = "5g")  # connect to H2O instance

# convert data to h2o object
my_basket.h2o <- as.h2o(my_basket)
```

```{r}
# run PCA
my_pca <- h2o.prcomp(
  training_frame = my_basket.h2o,
  pca_method = "GramSVD",
  k = ncol(my_basket.h2o), 
  transform = "STANDARDIZE", 
  impute_missing = TRUE,
  max_runtime_secs = 1000)
my_pca
```

#### Question: Why is the pca_method of “GramSVD” chosen over “GLRM”?

**GramSVD is the default setting for `pca_method`. GramSVD is better for numerical data, while GLRM is better for categorical data.**

#### Question: How many inital principal components are chosen with respect to dimensions of the input data? 

**42 components are chosen (the same as the number of features).**

```{r}
my_pca@model$eigenvectors %>% 
  as.data.frame() %>% 
  mutate(feature = row.names(.)) %>%
  ggplot(aes(pc1, reorder(feature, pc1))) +
  geom_point()
```

#### Question: What category of grocery items contribute most to PC1? (These are related because they're bought most often together on a given grocery trip)

**Alcohol products contribute most (top 3) to PC1.**

```{r}
my_pca@model$eigenvectors %>% 
  as.data.frame() %>% 
  mutate(feature = row.names(.)) %>%
  ggplot(aes(pc1, pc2, label = feature)) +
  geom_text()
```

#### Question: What category of grocery items contribute the least to PC1 but positively towards PC2?

**Vegetables (carrots, potatoes, broccoli, peas, spinach, leeks) have the lowest contribution to PC1, but a positive contribution to PC2.**

```{r}
# Compute eigenvalues
eigen <- my_pca@model$importance["Standard deviation", ] %>%
  as.vector() %>%
  .^2
  
# Sum of all eigenvalues equals number of variables
sum(eigen)

# Find PCs where the sum of eigenvalues is greater than or equal to 1
which(eigen >= 1)
```

```{r}
# Extract PVE and CVE
ve <- data.frame(
  PC  = my_pca@model$importance %>% seq_along(),
  PVE = my_pca@model$importance %>% .[2,] %>% unlist(),
  CVE = my_pca@model$importance %>% .[3,] %>% unlist())

# Plot PVE and CVE
ve %>%
  tidyr::gather(metric, variance_explained, -PC) %>%
  ggplot(aes(PC, variance_explained)) +
  geom_point() +
  facet_wrap(~ metric, ncol = 1, scales = "free")
```

```{r}
# How many PCs required to explain at least 90% of total variability
min(which(ve$CVE >= 0.9))
```

#### Question: How many principal components would you include to explain 90% of the total variance?

**36**

```{r}
# Screee plot criterion
data.frame(
  PC  = my_pca@model$importance %>% seq_along,
  PVE = my_pca@model$importance %>% .[2,] %>% unlist()) %>%
  ggplot(aes(PC, PVE, group = 1, label = PC)) +
  geom_point() +
  geom_line() +
  geom_text(nudge_y = -.002)
```

#### Question: How many principal components to include up to the elbow of the PVE, i.e. the “elbow” before plateau of dimensions explaining the least variance?

**8**

#### Question: What are a couple of disadvantages to using PCA?

**PCA is very sensitive to outliers, and also can only describe linear interactions.**

### NMDS

```{r}
# vegetation and environment in lichen pastures from Vare et al (1995)
data("varespec") # species
data("varechem") # chemistry

varespec %>% tibble()
```

#### Question: What are the dimensions of the varespec data frame and what do rows versus columns represent?

**Columns represent each species of lichen (44 species), and rows represent sites (24 sites).**

```{r}
vare.dis <- vegdist(varespec)
vare.mds0 <- monoMDS(vare.dis)
stressplot(vare.mds0)
```

#### Question: The “stress” in a stressplot represents the difference between the observed inpnut distance versus the fitted ordination distance. How much better is the non-metric (i.e., NMDS) fit versus a linear fit (as with PCA) in terms of \(R^2\)?

****

```{r}

```

